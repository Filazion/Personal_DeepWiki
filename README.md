# ğŸ¤– AI-Powered Repository Assistant â€” From Data Ingestion to Deployment  

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-App-red?logo=streamlit)
![OpenAI](https://img.shields.io/badge/OpenAI-API-green?logo=openai)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## ğŸ§  Overview  
I designed and implemented a complete **document-aware AI assistant** that can ingest any GitHub repository, index its documentation, and answer natural-language questions with file-level citations.  
This system brings together **data ingestion, intelligent chunking, hybrid search, and LLM-driven reasoning** into one deployable Streamlit application.

---

## ğŸ¯ Problem Statement  
Developers and researchers often spend hours searching large codebases and documentation.  
Traditional keyword search doesnâ€™t capture meaning or context â€” it canâ€™t answer questions like *â€œWhere is the authentication flow defined?â€* or *â€œWhat does this API function do?â€*  

I wanted to build an AI assistant that could:  
âœ… Understand documentation contextually  
âœ… Retrieve precise content chunks from repositories  
âœ… Generate accurate, cited answers in real time  

---

## âš™ï¸ Key Features  
- ğŸ§© **GitHub ingestion pipeline** â€” downloads and parses repository markdown files via `Requests` + `Python-Frontmatter`.  
- ğŸ§  **Multi-strategy chunking** â€” sliding window, paragraph-based, and section-based splits to preserve meaning.  
- ğŸ” **Hybrid search engine** â€” combines lexical search (`minsearch`) and semantic embeddings (`sentence-transformers`).  
- ğŸ¤ **LLM-driven agent** â€” built using `Pydantic-AI` and OpenAIâ€™s function-calling API with enforced citations.  
- ğŸ§¾ **Evaluation framework** â€” LLM-as-a-Judge scoring relevance, clarity, and completeness.  
- ğŸ’¬ **Streamlit web app** â€” interactive interface with streaming responses, logs, and persistent chat history.  
- ğŸ“¦ **Modular package design** â€” all logic separated into clean scripts (`ingest.py`, `search_tools.py`, `search_agent.py`, `logs.py`, `app.py`).  

---

## ğŸ§© Architecture  

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚             GitHub Repo                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
             [ Data Ingestion Pipeline ]
               - Requests
               - Python-Frontmatter
                         â”‚
                         â–¼
           [ Intelligent Text Chunking ]
               - Section, paragraph, sliding
                         â”‚
                         â–¼
             [ Hybrid Search Index ]
         - minsearch (lexical)
         - sentence-transformers (semantic)
                         â”‚
                         â–¼
             [ AI Agent (Pydantic-AI) ]
         - OpenAI Function-Calling
         - Citation Enforcement
                         â”‚
                         â–¼
            [ Evaluation & Logging Layer ]
               - LLM-as-a-Judge
               - JSON logs via Pandas
                         â”‚
                         â–¼
            [ Streamlit Web Interface ]
               - Real-time Q&A
               - Chat history
               - API config

---

## ğŸ§® Approach  

1. **Data Ingestion**  
   - Downloaded and extracted markdown documentation directly from GitHub zip archives.  
   - Parsed metadata using `python-frontmatter` for file organization.  

2. **Chunking & Indexing**  
   - Implemented multiple chunking methods to prevent context loss.  
   - Indexed chunks via a **hybrid search** pipeline combining lexical (`minsearch`) and semantic (`sentence-transformers`) retrieval.  

3. **Agent Construction**  
   - Integrated **Pydantic-AI** for structured prompt handling and OpenAI function-calling.  
   - Enforced mandatory search before answering and added file-name citation requirement.  

4. **Evaluation & Logging**  
   - Built an LLM evaluation system (â€œLLM-as-a-Judgeâ€) to automatically score each output.  
   - Logged query performance metrics (accuracy, latency, relevance) to JSON for Pandas analysis.  

5. **Deployment**  
   - Developed an interactive Streamlit dashboard with chat-based interface and streaming responses.  
   - Packaged as modular scripts for scalability and reuse across other repositories.  

---

## ğŸš€ Results  
| Metric | Value |
|--------|--------|
| **Repositories Processed** | [xx] |
| **Documents Indexed** | [xx] |
| **Average Evaluation Score** | [xx]% |
| **Query Latency** | [xx] seconds |

âœ… Delivered a **production-ready assistant** capable of answering repository questions with full traceability.  
âœ… Demonstrated [xx]% accuracy in identifying correct file sources.  
âœ… Achieved significant improvements in search relevance over baseline keyword methods.

---

## ğŸ–¥ï¸ Screenshots  

| Screenshot | Description |
|-------------|--------------|
| ![Dashboard](./ai-repo-assistant-dashboard.png) | Streamlit interface showing real-time answers with citations |
| ![Architecture](./ai-repo-assistant-architecture.png) | System pipeline from ingestion to deployment |
| ![Evaluation](./ai-repo-assistant-evals.png) | Evaluation results summarized with Pandas |

---

## ğŸ§° Tech Stack  
| Tool / Library | Purpose |
|-----------------|----------|
| **Python** | Core programming language |
| **OpenAI API** | LLM responses + function-calling |
| **Pydantic-AI** | Tool orchestration & structured prompts |
| **minsearch** | Lightweight lexical search engine |
| **sentence-transformers** | Semantic embeddings for retrieval |
| **Streamlit** | Web interface & app hosting |
| **Pandas** | Data logging and evaluation |
| **Requests** | GitHub API integration |
| **Python-Frontmatter** | Markdown metadata parsing |

---

## âš™ï¸ Installation & Usage  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/[yourusername]/ai-repo-assistant.git
cd ai-repo-assistant
